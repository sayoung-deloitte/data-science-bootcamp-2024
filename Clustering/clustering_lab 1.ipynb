{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65eb8ec2",
   "metadata": {},
   "source": [
    "<b> Note: This notebook has been run using python 3.9.5 </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check you python version uncomment below:\n",
    "\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261ec32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "from itertools import cycle, islice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae2211",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "Clustering is an unsupervised learning technique that aims to make sense of the underlying structure and pattern of unlabelled data. The task is to group data that are most similar together, and data from different groups should be highly dissimilar in traits.\n",
    "\n",
    "In this Notebook example, we'll explore how the four clustering algorithms (Agglomerativet, K-means, DBSCAN, EM for GMM) learnt in the pack clusters different types of datasets. Below are the datasets that we will cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8de39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles, make_moons, make_blobs, make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Create and plot the noisy circles dataset\n",
    "circles_data, circles_labels = make_circles(n_samples=1500, factor=0.5, noise=0.05)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(141) # (nrows, ncols, index)\n",
    "plt.scatter(circles_data[:, 0] # all the rows, first column (x)\n",
    "            , circles_data[:, 1] # all the rows, second column (y)\n",
    "            , c=circles_labels #  a scalar or sequence of n numbers to be mapped to colors using cmap and norm. Note as norm isn't in the parameters: By default, a linear scaling is used, mapping the lowest value to 0 and the highest to 1.\n",
    "            , cmap='viridis' # Colormap instance or registered colormap name used to map scalar data to colors. \n",
    "            , s=5) # marker size\n",
    "plt.title(\"Noisy Circles\")\n",
    "plt.axis('equal')\n",
    "\n",
    "# Create and plot the noisy moons dataset\n",
    "moons_data, moons_labels = make_moons(n_samples=1500, noise=0.1)\n",
    "plt.subplot(142)\n",
    "plt.scatter(moons_data[:, 0], moons_data[:, 1], c=moons_labels, cmap='viridis', s=5)\n",
    "plt.title(\"Noisy Moons\")\n",
    "plt.axis('equal')\n",
    "\n",
    "# Create and plot the noisy blobs dataset\n",
    "blobs_data, blobs_labels = make_blobs(n_samples=1500, centers=2, cluster_std=1.0, random_state=42)\n",
    "plt.subplot(143)\n",
    "plt.scatter(blobs_data[:, 0], blobs_data[:, 1], c=blobs_labels, cmap='viridis', s=5)\n",
    "plt.title(\"Noisy Blobs\")\n",
    "plt.axis('equal')\n",
    "\n",
    "# Create and plot the noisy aniso dataset\n",
    "aniso_data, aniso_labels = make_classification(n_samples=1500, n_features=2, n_informative=2,\n",
    "                                              n_redundant=0, n_clusters_per_class=1,\n",
    "                                              weights=[0.1, 0.9], flip_y=0.1, random_state=42)\n",
    "plt.subplot(144)\n",
    "plt.scatter(aniso_data[:, 0], aniso_data[:, 1], c=aniso_labels, cmap='viridis', s=5)\n",
    "plt.title(\"Noisy Aniso\")\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f7bb4d",
   "metadata": {},
   "source": [
    "## 1. Agglomerative Clustering\n",
    "Falling under Hierarchical Clustering, this uses a bottom-top approach to group data based on distance proximity to each other â€“smaller distance is considered more similar. <p>\n",
    "sk.learn.cluster.AgglomerativeCLustering package supports the single, complete, average and ward linkage method. In this example, we will use euclidean as the distance calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966f5f8d",
   "metadata": {},
   "source": [
    "- Apply Agglomerative clustering on all 4 datasets and plot results.\n",
    "- Try using different linkage methods such as simple, complete, average, ward\n",
    "\n",
    "Hint: one example is done for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b95e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for one dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "plt.figure(figsize=(30, 5))\n",
    "plt.tight_layout(w_pad=3.0)\n",
    "\n",
    "# Define a function to apply Agglomerative Clustering with the specified linkage method\n",
    "def apply_and_plot_agglomerative(linkage, subplot):\n",
    "    clustering = AgglomerativeClustering(n_clusters=2, linkage=linkage)\n",
    "    agglomerative_clusters = clustering.fit_predict(circles_data)\n",
    "\n",
    "    plt.subplot(1, 4, subplot)\n",
    "    plt.scatter(circles_data[:, 0], circles_data[:, 1], c=agglomerative_clusters, cmap='viridis', s=5)\n",
    "    plt.title(f'Agglomerative ({linkage})')\n",
    "    plt.axis('equal')\n",
    "\n",
    "# Apply and plot Agglomerative Clustering with different linkage methods\n",
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "for i, linkage in enumerate(linkage_methods, 1):\n",
    "    apply_and_plot_agglomerative(linkage, i)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO TRY YOURSELF: Fill in the TO ADD sections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.tight_layout(w_pad=3.0)\n",
    "\n",
    "# Define a function to apply Agglomerative Clustering with the specified linkage method on given datasets\n",
    "def apply_and_plot_agglomerative(linkage_methods, dataset):\n",
    "\n",
    "    for subplot, linkage in enumerate(linkage_methods, 1):\n",
    "        for i, data in enumerate(dataset, 1):\n",
    "            # TO ADD:  function to apply Agglomerative Clustering with the specified linkage method & predict specified dataset\n",
    "\n",
    "            \n",
    "            plt.subplot(, , (i-1)*4+subplot) #TO ADD: nrows = no. of linkage methods, ncols = no. of datasets. HINT: use len()\n",
    "            if i == 1:\n",
    "                plt.title(f'Agglomerative ({linkage})')\n",
    "            plt.scatter(data[:, 0], data[:, 1], c=agglomerative_clusters, cmap='viridis', s=5)\n",
    "            plt.axis('equal')\n",
    "\n",
    "# Apply and plot Agglomerative Clustering with different linkage methods on different datasets\n",
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "dataset = [] #TO ADD: the variables for the 4 different datasets we've created earlier\n",
    "\n",
    "apply_and_plot_agglomerative(linkage_methods, dataset)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e808d77",
   "metadata": {},
   "source": [
    "# 2. K-Means Clustering\n",
    "This is a popular clustering technique that falls under Centroid-based clustering. This iterative method aims to partition data points into k-number of clusters, grouping similar data points together to minimise variance within clusters. For K-Means clustering, each data point only belongs to one cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a514e03",
   "metadata": {},
   "source": [
    "- Apply Kmeans clustering on all 4 datasets and plot results.\n",
    "- Try using different parameters such as \"init\", \"n_init\", \"max_iter\", \"random_state\". Search google on what each parameter does.\n",
    "\n",
    "Hint: one example is done for you, and the second is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b599c12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example for one dataset\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define the number of clusters (in this case, 2 for the circles dataset)\n",
    "n_clusters = 2\n",
    "\n",
    "# Apply K-Means clustering to the circles dataset\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans_clusters = kmeans.fit_predict(circles_data)\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(circles_data[:, 0], circles_data[:, 1], c=kmeans_clusters, cmap='viridis', s=5)\n",
    "plt.title(f'K-Means Clustering (K={n_clusters})')\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO TRY YOURSELF: Fill in the TO ADD sections\n",
    "# moons_data with some parameters\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define the number of clusters (in this case, 2 for given dataset)\n",
    "n_clusters = 2\n",
    "\n",
    "# Define K-Means clustering parameters \n",
    "# TO ADD: look up what these parameters mean and play around with changing the parameters and look at the result\n",
    "init_method = \n",
    "n_init = \n",
    "max_iter = \n",
    "random_state = \n",
    "\n",
    "# Apply K-Means clustering to the moons_data dataset\n",
    "kmeans = KMeans(n_clusters=n_clusters, init=init_method, n_init=n_init, max_iter=max_iter, random_state=random_state)\n",
    "kmeans_clusters = kmeans.fit_predict(moons_data)\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(moons_data[:, 0], moons_data[:, 1], c=kmeans_clusters, cmap='viridis', s=5)\n",
    "plt.title(f'K-Means Clustering (K={n_clusters})')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af724c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: for blobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa22ea19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: for aniso_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45024d41",
   "metadata": {},
   "source": [
    "## 3. DBSCAN\n",
    "Density-based Spatial Clustering of Application with Noise (DBSCAN) is a density-based clustering method that requires two parameters:<p>\n",
    "- Epsilon: radius to create a circle from a datapoint, forming a perimeter around that datapoint\n",
    "- minPoints: minimum number of data points inside a circle (including the data point that the circle is created around) that must exist for a cluster to be formed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b00f073",
   "metadata": {},
   "source": [
    "- Apply DBSCAN clustering on all 4 datasets and plot results.\n",
    "- Try changing the value of \"eps\" and \"min_samples\" and observe output\n",
    "- Try using different parameters (changing values) such as \"metric\", \"algorithm\". Search google on what each parameter does.\n",
    "\n",
    "Hint: one example is done for you, and the second is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98c662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example with one dataset\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Apply DBSCAN clustering to the circles dataset\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=20)\n",
    "dbscan_clusters = dbscan.fit_predict(circles_data)\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(circles_data[:, 0], circles_data[:, 1], c=dbscan_clusters, cmap='viridis', s=5)\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13bb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO TRY YOURSELF: Fill in the TO ADD sections\n",
    "# moons_data with various parameters\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Define DBSCAN clustering parameters\n",
    "# TO ADD: look up what these parameters mean and play around with changing the parameters and look at the result\n",
    "eps_values =\n",
    "min_samples_values =\n",
    "metric = \n",
    "algorithm = \n",
    "\n",
    "# Iterate through different parameter combinations\n",
    "for eps in eps_values:\n",
    "    for min_samples in min_samples_values:\n",
    "        # Apply DBSCAN clustering to the circles dataset\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric=metric, algorithm=algorithm)\n",
    "        dbscan_clusters = dbscan.fit_predict(moons_data)\n",
    "\n",
    "        # Plot the clustered data\n",
    "        plt.figure(figsize=(4, 3))\n",
    "        plt.scatter(moons_data[:, 0], moons_data[:, 1], c=dbscan_clusters, cmap='gist_rainbow', s=5)\n",
    "        plt.title(f\"DBSCAN Clustering (eps={eps}, min_samples={min_samples})\")\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2facdf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: for blobs_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443802d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: for aniso_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83028f59",
   "metadata": {},
   "source": [
    "## 4. Expectation-Maximization (EM) Algorithm for Gaussian Mixture Method (GMM)\n",
    "Expectation-Maximization (EM) Algorithm for Gaussian Mixture Method (GMM)\n",
    "is a distribution-based clustering method that assumes the data points are Gaussian distributed and each Gaussian distribution is assigned to a cluster. This uses Expectation-Maximization algorithm to find the optimised parameters of the Gaussian distribution for each cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a6b1d7",
   "metadata": {},
   "source": [
    "- Apply EM for GMM clustering on all 4 datasets and plot results.\n",
    "- Try changing the value for n_components and observe the output\n",
    "- Try using different parameters such as \"covariance_type\", \"max_iter\", \"init_params\". Search google on what each parameter does.\n",
    "\n",
    "Hint: one example is done for you, and the second is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa5cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple example for one dataset\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Apply EM algorithm for GMM clustering to the circles dataset\n",
    "n_components = 2  # Number of Gaussian components\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "gmm_clusters = gmm.fit_predict(circles_data)\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(circles_data[:, 0], circles_data[:, 1], c=gmm_clusters, cmap='viridis', s=5)\n",
    "plt.title(\"Gaussian Mixture Model (GMM) Clustering\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68205c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO TRY YOURSELF: Fill in the TO ADD sections\n",
    "# moons_data\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Define GMM clustering parameters\n",
    "# TO ADD: look up what these parameters mean and play around with changing the parameters and look at the result\n",
    "n_components =\n",
    "covariance_type =\n",
    "max_iter =\n",
    "init_params = \n",
    "\n",
    "# Apply GMM clustering to the circles dataset\n",
    "gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, max_iter=max_iter, init_params=init_params, random_state=0)\n",
    "gmm_clusters = gmm.fit_predict(moons_data)\n",
    "\n",
    "# Plot the clustered data\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(moons_data[:, 0], moons_data[:, 1], c=gmm_clusters, cmap='viridis', s=5)\n",
    "plt.title(\"Gaussian Mixture Model (GMM) Clustering\")\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: for blobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2484732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: for aniso_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb9d860",
   "metadata": {},
   "source": [
    "## 5. Comparison\n",
    "Below shows the comparison of the four algorithms we have used. Alter the parameters for each of the algorithms to see how it affects the clusters formed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0eb75",
   "metadata": {},
   "source": [
    "- Compare all 4 algorithms for all 4 datasets and decide which algorithms are best for what kind of data\n",
    "\n",
    "First one is done for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb36a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algorithms\n",
    "n_clusters = 2\n",
    "#Agglomerative Clustering\n",
    "clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='single') # TO ALTER: change the linkage to see result for that linkage\n",
    "agglomerative_clusters = clustering.fit_predict(circles_data)\n",
    "\n",
    "#K-Means\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans_clusters = kmeans.fit_predict(circles_data)\n",
    "\n",
    "#DBSCAN\n",
    "dbscan = DBSCAN(eps=0.2, min_samples=20)\n",
    "dbscan_clusters = dbscan.fit_predict(circles_data)\n",
    "\n",
    "#EM FOR GMM\n",
    "gmm = GaussianMixture(n_components=n_components, random_state=0)\n",
    "gmm_clusters = gmm.fit_predict(circles_data)\n",
    "\n",
    "# Create subplots to compare the clustering results\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# K-Means\n",
    "plt.subplot(221)\n",
    "plt.scatter(circles_data[:, 0], circles_data[:, 1], c=agglomerative_clusters, cmap='viridis', s=5)\n",
    "plt.title(\"Agglomerative Clustering\")\n",
    "plt.axis('equal')\n",
    "\n",
    "# Agglomerative Clustering\n",
    "plt.subplot(222)\n",
    "plt.scatter(circles_data[:, 0], circles_data[:, 1], c=kmeans_clusters, cmap='viridis', s=5)\n",
    "plt.title(\"Kmeans Clustering\")\n",
    "plt.axis('equal')\n",
    "\n",
    "# DBSCAN\n",
    "plt.subplot(223)\n",
    "plt.scatter(circles_data[:, 0], circles_data[:, 1], c=dbscan_clusters, cmap='viridis', s=5)\n",
    "plt.title(\"DBSCAN Clustering\")\n",
    "plt.axis('equal')\n",
    "\n",
    "# GMM\n",
    "plt.subplot(224)\n",
    "plt.scatter(circles_data[:, 0], circles_data[:, 1], c=gmm_clusters, cmap='viridis', s=5)\n",
    "plt.title(\"GMM Clustering\")\n",
    "plt.axis('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c072748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: for moons_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b01578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: for blobs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8106ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO ADD: for aniso_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
