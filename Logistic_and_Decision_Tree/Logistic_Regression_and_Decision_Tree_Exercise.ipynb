{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression\n",
        "\n",
        "Logistic Regression is a statistical method used for binary classification problems in machine learning. Despite its name, logistic regression is primarily used for classification rather than regression tasks. It models the probability of the occurrence of a binary outcome by fitting data to a logistic function, also known as the sigmoid function.\n"
      ],
      "metadata": {
        "id": "oY2rUnYe5dva"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYnqKWcWkd5h"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestRegressor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        "\n",
        "1. Read the csv file.\n",
        "2. Drop column id as it is not required.\n",
        "3. Find the description of the data.\n",
        "4. Find if any null values exist for any column and remove the null values.\n"
      ],
      "metadata": {
        "id": "MlEI67FN8loM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the health_dataset_stroke_data.csv file and display it's content\n"
      ],
      "metadata": {
        "id": "1uXtd2tb5eoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop column id from the dataframe\n"
      ],
      "metadata": {
        "id": "AGfMQP-4I6WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the description of the data\n"
      ],
      "metadata": {
        "id": "1_5jf_eJ5eqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the null values for each variable\n"
      ],
      "metadata": {
        "id": "rp-W7aHl5etC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the null values\n"
      ],
      "metadata": {
        "id": "hnvH0dnK6kr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the shape of dataframe\n",
        "df.shape"
      ],
      "metadata": {
        "id": "dSE3MjSeDoFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "\n",
        "1. Plot Scatter Matrix Plot from data\n",
        "2. Plot Histogram plot"
      ],
      "metadata": {
        "id": "8EwJhvbr5eFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Scatter Matrix plot\n"
      ],
      "metadata": {
        "id": "TCwK63iyDp2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Histogram plot of numeric variables\n"
      ],
      "metadata": {
        "id": "Bkv1kEOZDxEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3\n",
        "\n",
        "Create a confusion plot and look if you see correlation amoung indepedent(input) variable."
      ],
      "metadata": {
        "id": "5uDzrfjI5uaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the correlation map\n",
        "\n",
        "#Define size ofthe plot\n",
        "\n",
        "#Plot heatmap of correlation between columns\n"
      ],
      "metadata": {
        "id": "UEGp9BRhD1Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "\n",
        "Create a bar plot of class to be predicted that is output variable Stroke. This will show whether class is imbalanced or not."
      ],
      "metadata": {
        "id": "g46jyvDB6Dcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the count of output variable stroke using bar chat\n"
      ],
      "metadata": {
        "id": "1_CNWF7tEWPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5: One Hot Encoding\n",
        "Using One hot encoding convert categorical values into numeric that is 0 and 1."
      ],
      "metadata": {
        "id": "WlkK9VS0Ixfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform one hot encoding on categorical variables\n",
        "\n",
        "# first convert categorical variables to dummy variables using one hot encoding\n",
        "categorical_var =\n",
        "\n",
        "# create dummy variables for all the other categorical variables\n",
        "for variable in categorical_var:\n",
        "#   create dummy variables for given columns\n",
        "    dummies =\n",
        "    df =\n",
        "    df.drop([variable],axis=1,inplace=True)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6AcoFbliGIWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 6: Building the Logistic Regression Model and Decision Tree\n",
        "\n",
        "1. Split data into Training and Testing data.\n",
        "2. Create Instance of Logistci Regression and fit the model on train.\n",
        "3. Predict the value of Test data.\n",
        "4. Calculate Accuracy, F1 Score, Recall, and Precision\n",
        "5. Create a dataframe having Actual and Predicted Values"
      ],
      "metadata": {
        "id": "ZO12WLzV6wIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data in test and training\n",
        "\n",
        "# Building the model\n",
        "\n",
        "# split train and test dataset\n",
        "\n",
        "\n",
        "print(\"Training input variable size:\", train_x.shape)\n",
        "print(\"Training output variable size\", train_y.shape)\n",
        "\n",
        "print(\"Testing input variable size:\", test_x.shape)\n",
        "print(\"Testing input variable size:\", test_y.shape)"
      ],
      "metadata": {
        "id": "5JB7kVCyJeTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Logistic regression model, fit, predict and evaluate the score of the model\n",
        "\n",
        "#Create an instance of linear model\n",
        "lr =\n",
        "#Fit the model to data\n",
        "model =\n",
        "\n",
        "# Model Score\n",
        "score =\n",
        "\n",
        "# Predict the value of text_x data\n",
        "pred_y =\n",
        "\n",
        "# Print the score\n",
        "print('Score: ', score)"
      ],
      "metadata": {
        "id": "3pkPEYQEJn7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy, Recall, Precision, and F1 Score\n",
        "\n",
        "Accuracy, Recall, Precision, and F1 Score are metrics commonly used to evaluate the performance of classification models. These metrics provide insights into different aspects of a model's effectiveness in making predictions on a given dataset.\n",
        "\n",
        "1. Accuracy:\n",
        "\n",
        "Formula:\n",
        "Accuracy\n",
        "=\n",
        "Number of Correct Predictions /\n",
        "Total Number of Predictions​\n",
        "\n",
        "Interpretation: Accuracy measures the overall correctness of the model by calculating the ratio of correctly predicted instances to the total number of instances. While accuracy is a commonly used metric, it may not be suitable for imbalanced datasets.\n",
        "\n",
        "2. Recall (Sensitivity or True Positive Rate):\n",
        "\n",
        "Formula:\n",
        "Recall\n",
        "=\n",
        "True Positives /\n",
        "(True Positives + False Negatives)\n",
        "\n",
        "Interpretation: Recall evaluates the ability of a model to correctly identify all relevant instances (true positives) out of the total instances that actually belong to the positive class. It is particularly important in situations where missing positive instances (false negatives) is a critical concern.\n",
        "\n",
        "3. Precision (Positive Predictive Value):\n",
        "\n",
        "Formula:\n",
        "Precision\n",
        "=\n",
        "True Positives /\n",
        "(True Positives + False Positives​)\n",
        "\n",
        "Interpretation: Precision measures the accuracy of positive predictions by calculating the ratio of true positives to the total predicted positives. It is valuable in scenarios where the cost of false positives is high.\n",
        "\n",
        "4. F1 Score:\n",
        "\n",
        "Formula:\n",
        "F1 Score = 2×(Precision×Recall) /\n",
        "(Precision + Recall)\n",
        "\n",
        "Interpretation: The F1 Score is the harmonic mean of precision and recall. It provides a balance between precision and recall and is useful when there is an uneven class distribution. The F1 Score reaches its best value at 1 (perfect precision and recall) and worst at 0."
      ],
      "metadata": {
        "id": "ylltEtZmVw3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the Accuracy, Recall, F1 Score and Precision\n",
        "\n",
        "print('Accuracy: '  + str(metrics.accuracy_score(test_y, pred_y)))\n",
        "print('Recall: ' + str(metrics.recall_score(test_y, pred_y)))\n",
        "print('F1 Score: ' + str(metrics.f1_score(test_y, pred_y)))\n",
        "print('Precision: ' + str(metrics.precision_score(test_y, pred_y)))"
      ],
      "metadata": {
        "id": "WxW5SAqvJofy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix\n",
        "\n",
        "A confusion matrix is a table that is often used to evaluate the performance of a classification algorithm. It summarizes the predictions of a model on a classification problem in terms of true positive (TP), true negative (TN), false positive (FP), and false negative (FN) outcomes. The matrix is especially useful when dealing with imbalanced datasets.\n",
        "\n",
        "True Positive (TP): Instances that are actually positive and are correctly predicted as positive.\n",
        "\n",
        "True Negative (TN): Instances that are actually negative and are correctly predicted as negative.\n",
        "\n",
        "False Positive (FP): Instances that are actually negative but are incorrectly predicted as positive.\n",
        "\n",
        "False Negative (FN): Instances that are actually positive but are incorrectly predicted as negative."
      ],
      "metadata": {
        "id": "0y-lWogtX8IF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = metrics.confusion_matrix(test_y, y_pred)\n",
        "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h9CAjAFfXDJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataframe to show Actual and Predicted values\n",
        "\n",
        "df_predicted = pd.DataFrame({'Actual': test_y, 'Predicted': np.round(pred_y,2)})\n",
        "df_predicted.head(30)"
      ],
      "metadata": {
        "id": "_KcIa1ksKyqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make Prediction using Decision Tree"
      ],
      "metadata": {
        "id": "HuSurj6m3mpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Prediction Using Desicion Tree\n",
        "\n",
        "\n",
        "# Train the model on the training set\n",
        "\n",
        "\n",
        "# Make predictions on the test set\n",
        "\n"
      ],
      "metadata": {
        "id": "T7vz2SfSYvXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the Accuracy, Recall, F1 Score and Precision\n",
        "\n"
      ],
      "metadata": {
        "id": "ObEtlQ1mYvTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Confusion Matrxi\n",
        "\n",
        "cm =\n",
        "disp =\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Zm9DSH-8YvRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_predicted =\n",
        "df_predicted.head(30)"
      ],
      "metadata": {
        "id": "Q2vQou4mYvPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Imbalanced Datasets\n",
        "\n",
        "Handling imbalanced datasets is an important consideration in machine learning, especially in classification tasks. An imbalanced dataset occurs when the distribution of classes is not equal, and one class has significantly fewer samples than the others. Here are few techniques to address imbalanced datasets:\n",
        "\n",
        "1. Under-sampling: Reduce the size of the majority class to match the minority class by randomly removing instances. This can help balance the class distribution but may result in loss of information.\n",
        "\n",
        "  Over-sampling: Increase the size of the minority class by duplicating instances or generating synthetic examples (e.g., using techniques like SMOTE - Synthetic Minority Over-sampling Technique). This can help balance the class distribution but may lead to overfitting.\n",
        "\n",
        "3. Generate Synthetic Samples:\n",
        "Techniques like SMOTE (Synthetic Minority Over-sampling Technique) can generate synthetic samples for the minority class, helping to balance the class distribution."
      ],
      "metadata": {
        "id": "4VPsTZ1iwXw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Under Sampling\n",
        "\n",
        "# select all rows having lable value 0\n",
        "no_stroke = df[df['stroke'] == 0]\n",
        "# select all rows having lable value 1\n",
        "stroke = df[df['stroke'] == 1]\n",
        "\n",
        "# create a limiter\n",
        "limit = len(stroke) * 6\n",
        "\n",
        "# limit the number of data points\n",
        "no_stroke = no_stroke.sample(limit)\n",
        "# Concat the data for 0 and 1\n",
        "test_resample = pd.concat([no_stroke, stroke])\n",
        "test_resample"
      ],
      "metadata": {
        "id": "nNpIQOp81Tif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_resample_x = test_resample.drop(['stroke'], axis=1)\n",
        "train_resample_y = test_resample['stroke']"
      ],
      "metadata": {
        "id": "pRR9yF9w1TfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Logistic regression model, fit, predict and evaluate the score of the model\n",
        "\n",
        "#Create an instance of linear model\n",
        "lr = LogisticRegression()\n",
        "#Fit the model to data\n",
        "model = lr.fit(train_resample_x, train_resample_y)\n",
        "\n",
        "# Model Score\n",
        "score = model.score(test_x,test_y)\n",
        "\n",
        "# Predict the value of text_x data\n",
        "pred_y = lr.predict(test_x)\n",
        "\n",
        "# Print the score\n",
        "print('Score: ', score)"
      ],
      "metadata": {
        "id": "xo-9w6iY268I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the Accuracy, Recall, F1 Score and Precision\n",
        "\n",
        "print('Accuracy: '  + str(metrics.accuracy_score(test_y, pred_y)))\n",
        "print('Recall: ' + str(metrics.recall_score(test_y, pred_y)))\n",
        "print('F1 Score: ' + str(metrics.f1_score(test_y, pred_y)))\n",
        "print('Precision: ' + str(metrics.precision_score(test_y, pred_y)))"
      ],
      "metadata": {
        "id": "rg1sv16z264x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Confusion Matrix\n",
        "cm = metrics.confusion_matrix(test_y, pred_y)\n",
        "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1FnYlSRA26uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make Prediction Using Desicion Tree\n",
        "\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model on the training set\n",
        "clf.fit(train_resample_x, train_resample_y)\n",
        "\n",
        "# Make predictions on the test set\n",
        "pred_y = clf.predict(test_x)\n"
      ],
      "metadata": {
        "id": "_ZhDpkiW3s7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# find the Accuracy, Recall, F1 Score and Precision\n",
        "\n",
        "print('Accuracy: '  + str(metrics.accuracy_score(test_y, pred_y)))\n",
        "print('Recall: ' + str(metrics.recall_score(test_y, pred_y)))\n",
        "print('F1 Score: ' + str(metrics.f1_score(test_y, pred_y)))\n",
        "print('Precision: ' + str(metrics.precision_score(test_y, pred_y)))"
      ],
      "metadata": {
        "id": "QsRW4GBZ3s3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create confusion Matrix\n",
        "\n",
        "cm = metrics.confusion_matrix(test_y, pred_y)\n",
        "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3ObCemP_3s1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMR4aQJczi-6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}